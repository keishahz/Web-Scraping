{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Web Scraping Class Central dengan Selenium**\n",
        "---\n",
        "Pada bagian ini, akan dilakukan web scraping dari situs [Class Central](https://www.classcentral.com/subject/cs) menggunakan **Selenium**.\n",
        "\n",
        "Tujuan scraping ini adalah untuk mengambil data kursus seperti judul, provider, rating, bahasa, ketersediaan sertifikat, dan status gratis/berbayar.\n"
      ],
      "metadata": {
        "id": "HqrlaYSUXqBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install semua library yang dibutuhkan\n",
        "\n",
        "!pip install requests\n",
        "!pip install selenium\n",
        "!pip install -q google-colab-selenium\n",
        "!pip install nltk\n",
        "!pip install selenium webdriver-manager pandas\n",
        "\n",
        "!apt-get update\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get install -f -y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T07:04:30.104748Z",
          "iopub.execute_input": "2025-07-01T07:04:30.105121Z",
          "iopub.status.idle": "2025-07-01T07:04:48.194161Z",
          "shell.execute_reply.started": "2025-07-01T07:04:30.105061Z",
          "shell.execute_reply": "2025-07-01T07:04:48.193185Z"
        },
        "id": "hbqKMm70zbuq",
        "outputId": "920185bf-8549-4e81-c526-fc167671b292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.34.2)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Hit:1 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2025-07-10 13:13:05--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.26.136, 74.125.26.190, 74.125.26.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.26.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118049688 (113M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.4’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 112.58M   300MB/s    in 0.4s    \n",
            "\n",
            "2025-07-10 13:13:05 (300 MB/s) - ‘google-chrome-stable_current_amd64.deb.4’ saved [118049688/118049688]\n",
            "\n",
            "(Reading database ... 126437 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (138.0.7204.100-1) over (138.0.7204.100-1) ...\n",
            "Setting up google-chrome-stable (138.0.7204.100-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import selenium\n",
        "import nltk\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import logging\n",
        "import string\n",
        "import json\n",
        "print(\"All libraries installed successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-01T07:04:48.195963Z",
          "iopub.execute_input": "2025-07-01T07:04:48.196755Z",
          "iopub.status.idle": "2025-07-01T07:04:48.226847Z",
          "shell.execute_reply.started": "2025-07-01T07:04:48.196723Z",
          "shell.execute_reply": "2025-07-01T07:04:48.225624Z"
        },
        "id": "6xE9ak8Xzbus",
        "outputId": "fe3ef892-2d21-4213-ed37-01ee00643ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries installed successfully!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Data Collection**"
      ],
      "metadata": {
        "id": "AvONXVcLzbus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install library terkait dan menyiapkan ChromeDriver di Google Colab.\n",
        "\n",
        "Karena Colab berjalan di server dan bukan di komputer lokal, kita perlu menjalankan Chrome dalam mode headless (tanpa tampilan GUI). Kita juga menggunakan `webdriver-manager` untuk mengelola ChromeDriver secara otomatis.\n"
      ],
      "metadata": {
        "id": "Xc9OTOFcoXjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "import copy\n",
        "import google_colab_selenium as gs"
      ],
      "metadata": {
        "id": "9MxfrdxCxWs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyiapkan konfigurasi logging untuk memantau proses scraping\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "VI9Dvmo-uL4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup():\n",
        "\n",
        "  # Fungsi untuk menyiapkan dan mengembalikan WebDriver (Chrome) di Google Colab.\n",
        "  # Menggunakan opsi headless dan beberapa flags tambahan agar dapat berjalan stabil.\n",
        "\n",
        "  logging.info(\"Menyiapkan WebDriver untuk lingkungan Google Colab...\")\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument(\"--headless\")  # Menjalankan Chrome tanpa GUI\n",
        "  chrome_options.add_argument(\"--no-sandbox\")\n",
        "  chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "  chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "  chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
        "\n",
        "  # Inisialisasi WebDriver menggunakan ChromeDriverManager\n",
        "  driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "  logging.info(\"WebDriver berhasil disiapkan.\")\n",
        "  return driver"
      ],
      "metadata": {
        "id": "oYGR2eb3t7n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data Class Central\n",
        "\n",
        "Mengambil data dari 10 halaman pertama pada kategori Computer Science di Class Central. Untuk setiap kursus, kita ambil informasi:\n",
        "- Judul kursus\n",
        "- Provider/platform\n",
        "- Bahasa\n",
        "- Sertifikat (tersedia/tidak)\n",
        "- Rating rata-rata\n",
        "- Status (gratis atau tidak)\n",
        "- Jumlah ulasan\n",
        "- Link ke kursus\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Menggunakan struktur HTML dan atribut `data-track-props` untuk mengekstrak informasi tersebut.\n"
      ],
      "metadata": {
        "id": "yQxeCRlMZYnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_classcentral(driver, base_url):\n",
        "\n",
        "    # Melakukan scraping dari halaman-halaman Class Central berdasarkan URL dasar.\n",
        "\n",
        "    all_courses_data = []\n",
        "    logging.info(f\"Memulai scraping dari: {base_url}\")\n",
        "\n",
        "    for page in range(1, 11):  # Halaman 1 sampai 10\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        logging.info(f\"Scraping halaman {page}: {url}\")\n",
        "        driver.get(url)\n",
        "        time.sleep(3)  # Jeda untuk memberi waktu halaman loading\n",
        "\n",
        "        # Cari semua elemen yang merupakan nama kursus\n",
        "        course_containers = driver.find_elements(By.CSS_SELECTOR, 'a.color-charcoal.course-name')\n",
        "        logging.info(f\"Halaman {page}: ditemukan {len(course_containers)} kursus.\")\n",
        "\n",
        "        for title_elem in course_containers:\n",
        "            try:\n",
        "                # Ambil teks judul dan link\n",
        "                title = title_elem.text.strip()\n",
        "                link = title_elem.get_attribute('href')\n",
        "\n",
        "                # Ambil atribut JSON tersembunyi untuk informasi tambahan\n",
        "                data_props_raw = title_elem.get_attribute('data-track-props')\n",
        "                data_props = json.loads(data_props_raw)\n",
        "\n",
        "                provider = data_props.get(\"course_provider\", \"Unknown\")\n",
        "                certificate = data_props.get(\"course_certificate\", False)\n",
        "                language = data_props.get(\"course_language\", \"N/A\")\n",
        "                avg_rating = data_props.get(\"course_avg_rating\", 0.0)\n",
        "                is_free = data_props.get(\"course_is_free\", False)\n",
        "\n",
        "                try:\n",
        "                    # Coba ambil teks ulasan\n",
        "                    reviews = title_elem.find_element(By.XPATH, '../..').find_element(By.CSS_SELECTOR, 'span.color-gray').text.strip()\n",
        "                except:\n",
        "                    reviews = \"0 reviews\"\n",
        "\n",
        "                # Simpan data ke list\n",
        "                all_courses_data.append({\n",
        "                    'title': title,\n",
        "                    'provider': provider,\n",
        "                    'language': language,\n",
        "                    'certificate': certificate,\n",
        "                    'avg_rating': avg_rating,\n",
        "                    'is_free': is_free,\n",
        "                    'reviews': reviews,\n",
        "                    'link': f\"https://www.classcentral.com{link}\"\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Error saat membaca 1 kursus di halaman {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Konversi hasil scraping ke DataFrame\n",
        "    return pd.DataFrame(all_courses_data)"
      ],
      "metadata": {
        "id": "V6Adti-1sgAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi WebDriver\n",
        "driver = setup()"
      ],
      "metadata": {
        "id": "a5CiIYjFy9er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL target kategori kursus Computer Science\n",
        "target_url = \"https://www.classcentral.com/subject/cs\"\n",
        "\n",
        "# Jalankan scraping dan simpan ke DataFrame\n",
        "scraped_data = scrape_classcentral(driver, target_url)"
      ],
      "metadata": {
        "id": "haIJhDqyAukS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Berhasil\n",
        "\n",
        "Scraping telah berhasil dan sudah diambil data dari 10 halaman kursus Class Central dan menyimpannya dalam bentuk DataFrame. Data ini selajutnya akan dibersihkan dan diolah pada bagian **Text Preprocessing**.\n"
      ],
      "metadata": {
        "id": "QscF594qaaVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Preprocessing (Text Cleaning)**"
      ],
      "metadata": {
        "id": "OPufYPKw8ncb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library Tambahan\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download resource NLTK untuk Bahasa Inggris\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Library berhasil di-import.\")"
      ],
      "metadata": {
        "id": "Kaf8nzFp_a1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5abe4ca-eae5-4880-8d9e-d6f23196383a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library berhasil di-import.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus URL, Hashtag, Emoji, Angka, dan Tanda Baca\n",
        "def clean_noise(text):\n",
        "\n",
        "  # Menghapus semua tag HTML secara utuh\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # Menghapus URL\n",
        "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "  # Menghapus Hashtag\n",
        "  text = re.sub(r'#\\w+', '', text)\n",
        "  # Menghapus Emoji dan Tanda Baca\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # Menghapus Angka\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "  # Menghapus spasi berlebih\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "DZqk2PdUCFTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus Stopwords\n",
        "\n",
        "# Define list_stopwords\n",
        "from nltk.corpus import stopwords\n",
        "list_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "  # Memecah kalimat menjadi kata-kata (tokenization)\n",
        "  tokens = text.split()\n",
        "\n",
        "  # Menghapus stopwords dari daftar token\n",
        "  tokens_without_stopwords = [word for word in tokens if word not in list_stopwords]\n",
        "\n",
        "  # Menggabungkan kembali token menjadi kalimat\n",
        "  text = ' '.join(tokens_without_stopwords)\n",
        "  return text"
      ],
      "metadata": {
        "id": "BTLOKtbgCOyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "# Membuat stemmer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "i_GWZ1xXCeO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pipeline**"
      ],
      "metadata": {
        "id": "XV1hiiorncqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_pipeline(text):\n",
        "  text = text.lower()\n",
        "  text = clean_noise(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = stemmer.stem(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "khpQbCaRnGo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menjalankan Pipeline Lengkap\n",
        "driver = setup()\n",
        "df = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    target_url = \"https://www.classcentral.com/subject/cs\"\n",
        "    df = scrape_classcentral(driver, target_url)\n",
        "\n",
        "    if not df.empty:\n",
        "        logging.info(\"\\nScraping berhasil. Memulai pipeline preprocessing...\")\n",
        "\n",
        "        # Terapkan Preprocessing ke Kolom Judul\n",
        "        text_before = df['title'].iloc[0] # Simpan contoh teks asli\n",
        "        df['cleaned_title'] = df['title'].apply(cleaning_pipeline)\n",
        "        logging.info(\"Pipeline preprocessing selesai.\")\n",
        "\n",
        "        print(\"\\n--- CONTOH HASIL PREPROCESSING (Data Pertama) ---\")\n",
        "        print(f\"\\n1. JUDUL ASLI:\\n{text_before}\")\n",
        "        print(f\"\\n2. HASIL AKHIR:\\n{df['cleaned_title'].iloc[0]}\")\n",
        "\n",
        "        # Format tambahan:\n",
        "        df['is_free'] = df['is_free'].map({True: 'Free', False: 'Paid'})\n",
        "        df['certificate'] = df['certificate'].map({True: 'Certificate Available', False: 'No Certificate'})\n",
        "        df['avg_rating'] = df['avg_rating'].apply(lambda x: f\"{x:.2f} ★\" if x != \"N/A\" else x)\n",
        "\n",
        "        # Susun kolom yang ingin disimpan\n",
        "        final_df = df[['title', 'provider', 'language', 'certificate', 'avg_rating', 'is_free', 'reviews', 'cleaned_title']]\n",
        "        final_df.columns = ['Title', 'Provider', 'Language', 'Certificate', 'Average Rating', 'Price Type', 'Reviews', 'Cleaned Title']\n",
        "\n",
        "        # Simpan file CSV dan JSON\n",
        "        output_file_csv = 'classcentral.csv'\n",
        "        final_df.to_csv(output_file_csv, index=False)\n",
        "        print(f\"\\nData bersih berhasil disimpan ke file: '{output_file_csv}'\")\n",
        "\n",
        "        output_file_json = 'classcentral.json'\n",
        "        final_df.to_json(output_file_json, orient='records', indent=4)\n",
        "        print(f\"Data bersih berhasil disimpan ke file: '{output_file_json}'\")\n",
        "\n",
        "    else:\n",
        "        logging.warning(\"Scraping tidak menghasilkan data. Tidak ada file yang disimpan.\")\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    logging.info(\"WebDriver ditutup.\")"
      ],
      "metadata": {
        "id": "WhKQ9O1fxIAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b9b059-ac70-4d74-b3a4-5a33bbf6eaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CONTOH HASIL PREPROCESSING (Data Pertama) ---\n",
            "\n",
            "1. JUDUL ASLI:\n",
            "CS50's Introduction to Computer Science\n",
            "\n",
            "2. HASIL AKHIR:\n",
            "css introduction computer sci\n",
            "\n",
            "Data bersih berhasil disimpan ke file: 'classcentral.csv'\n",
            "Data bersih berhasil disimpan ke file: 'classcentral.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proses selesai dan data siap digunakan!\n",
        "\n",
        "Seluruh pipeline telah berhasil dijalankan, meliputi:\n",
        "1. Data Collecting: Mengambil data kursus dari situs ClassCentral dengan menggunakan Selenium.\n",
        "2. Data Cleaning (Preprocessing):\n",
        "    * Konversi teks menjadi lowercase\n",
        "    * Penghapusan noise (URL, hashtag, angka, emoji, dan tanda baca)\n",
        "    * Penghapusan stopwords\n",
        "3. Data Export: Dataset yang sudah dibersihkan dan terstruktur telah disimpan dalam format CSV dan JSON."
      ],
      "metadata": {
        "id": "D_7YO2lNeohk"
      }
    }
  ]
}